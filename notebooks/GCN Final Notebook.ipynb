{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Label GCN Classifier\n",
    "\n",
    "## Approach:\n",
    "1. **Label Embeddings**: BERT encodes class keywords â†’ 531 label vectors\n",
    "2. **GCN Refinement**: Propagates info through hierarchy (siblings share info)\n",
    "3. **Inner Product**: `score = BERT(text) Â· GCN_refined(label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:23:55.646806Z",
     "iopub.status.busy": "2025-12-19T13:23:55.646681Z",
     "iopub.status.idle": "2025-12-19T13:24:00.710336Z",
     "shell.execute_reply": "2025-12-19T13:24:00.709546Z",
     "shell.execute_reply.started": "2025-12-19T13:23:55.646792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Device: cuda\n",
      "  GPU: NVIDIA L4\n",
      "CPU times: user 3.18 s, sys: 678 ms, total: 3.86 s\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip install -q transformers torch networkx scipy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import csv\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:02.021543Z",
     "iopub.status.busy": "2025-12-19T13:24:02.021347Z",
     "iopub.status.idle": "2025-12-19T13:24:02.025771Z",
     "shell.execute_reply": "2025-12-19T13:24:02.024719Z",
     "shell.execute_reply.started": "2025-12-19T13:24:02.021524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Config: GCN_LAYERS=2, TOP_K=20\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "DATA_DIR = 'data'\n",
    "OUTPUT_DIR = 'outputs'\n",
    "NUM_CLASSES = 531\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "HIDDEN_DIM = 768\n",
    "GCN_LAYERS = 2\n",
    "GCN_DROPOUT = 0.3\n",
    "TOP_K = 20\n",
    "MIN_SCORE = 0.1\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"âœ“ Config: GCN_LAYERS={GCN_LAYERS}, TOP_K={TOP_K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:03.407256Z",
     "iopub.status.busy": "2025-12-19T13:24:03.406960Z",
     "iopub.status.idle": "2025-12-19T13:24:03.412417Z",
     "shell.execute_reply": "2025-12-19T13:24:03.411694Z",
     "shell.execute_reply.started": "2025-12-19T13:24:03.407239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Load functions\n",
    "def load_corpus(path):\n",
    "    data = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                data[parts[0]] = parts[1]\n",
    "    return data\n",
    "\n",
    "def load_classes(path):\n",
    "    data = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                data[int(parts[0])] = parts[1]\n",
    "    return data\n",
    "\n",
    "def load_keywords(path):\n",
    "    data = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(':', 1)\n",
    "            if len(parts) == 2:\n",
    "                data[parts[0]] = [kw.strip() for kw in parts[1].split(',')]\n",
    "    return data\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    edges = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                edges.append((int(parts[0]), int(parts[1])))\n",
    "    return edges\n",
    "\n",
    "print(\"âœ“ Functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:04.951296Z",
     "iopub.status.busy": "2025-12-19T13:24:04.951104Z",
     "iopub.status.idle": "2025-12-19T13:24:04.977404Z",
     "shell.execute_reply": "2025-12-19T13:24:04.976834Z",
     "shell.execute_reply.started": "2025-12-19T13:24:04.951282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Test: 19,658\n",
      "âœ“ Classes: 531\n",
      "âœ“ Hierarchy edges: 568\n",
      "CPU times: user 24.4 ms, sys: 9.95 ms, total: 34.4 ms\n",
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load data\n",
    "test_corpus = load_corpus(os.path.join(DATA_DIR, 'test/test_corpus.txt'))\n",
    "id2class = load_classes(os.path.join(DATA_DIR, 'classes.txt'))\n",
    "class2keywords = load_keywords(os.path.join(DATA_DIR, 'class_related_keywords.txt'))\n",
    "hierarchy_edges = load_hierarchy(os.path.join(DATA_DIR, 'class_hierarchy.txt'))\n",
    "\n",
    "print(f\"âœ“ Test: {len(test_corpus):,}\")\n",
    "print(f\"âœ“ Classes: {NUM_CLASSES}\")\n",
    "print(f\"âœ“ Hierarchy edges: {len(hierarchy_edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:06.397737Z",
     "iopub.status.busy": "2025-12-19T13:24:06.397372Z",
     "iopub.status.idle": "2025-12-19T13:24:06.546980Z",
     "shell.execute_reply": "2025-12-19T13:24:06.546188Z",
     "shell.execute_reply.started": "2025-12-19T13:24:06.397707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building label graph...\n",
      "âœ“ A_hat: torch.Size([531, 531])\n",
      "  Edges: 3503\n",
      "CPU times: user 1.9 s, sys: 256 ms, total: 2.16 s\n",
      "Wall time: 145 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build adjacency matrix\n",
    "print(\"Building label graph...\")\n",
    "\n",
    "# Group children by parent\n",
    "parent2children = defaultdict(list)\n",
    "for parent, child in hierarchy_edges:\n",
    "    parent2children[parent].append(child)\n",
    "\n",
    "# Build graph - connect siblings\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(NUM_CLASSES))\n",
    "\n",
    "for parent, children in parent2children.items():\n",
    "    for i in range(len(children)):\n",
    "        for j in range(i+1, len(children)):\n",
    "            G.add_edge(children[i], children[j])\n",
    "\n",
    "# Add self-loops\n",
    "for i in range(NUM_CLASSES):\n",
    "    G.add_edge(i, i)\n",
    "\n",
    "# Convert to normalized adjacency\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "A = torch.FloatTensor(A)\n",
    "\n",
    "# D^{-1/2} * A * D^{-1/2}\n",
    "D = torch.sum(A, dim=1)\n",
    "D_inv_sqrt = torch.pow(D, -0.5)\n",
    "D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "D_mat = torch.diag(D_inv_sqrt)\n",
    "A_hat = D_mat @ A @ D_mat\n",
    "A_hat = A_hat.to(device)\n",
    "\n",
    "print(f\"âœ“ A_hat: {A_hat.shape}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:08.143537Z",
     "iopub.status.busy": "2025-12-19T13:24:08.143157Z",
     "iopub.status.idle": "2025-12-19T13:24:13.693490Z",
     "shell.execute_reply": "2025-12-19T13:24:13.692835Z",
     "shell.execute_reply.started": "2025-12-19T13:24:08.143507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating label embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:24:08.673211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766150648.684716   12272 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766150648.688345   12272 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-19 13:24:08.700419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:02<00:00, 181.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Label embeddings: torch.Size([531, 768])\n",
      "CPU times: user 5.04 s, sys: 448 ms, total: 5.49 s\n",
      "Wall time: 5.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1824"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create label embeddings\n",
    "print(\"Creating label embeddings...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "bert = AutoModel.from_pretrained(BERT_MODEL).to(device)\n",
    "bert.eval()\n",
    "\n",
    "label_embeds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(NUM_CLASSES), desc=\"Labels\"):\n",
    "        class_name = id2class[i]\n",
    "        keywords = class2keywords.get(class_name, [class_name.replace('_', ' ')])\n",
    "        text = ' '.join([kw.replace('_', ' ') for kw in keywords])\n",
    "        \n",
    "        enc = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                       max_length=128, padding=True)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        \n",
    "        out = bert(**enc)\n",
    "        emb = out.last_hidden_state.mean(dim=1).squeeze()\n",
    "        label_embeds.append(emb.cpu())\n",
    "\n",
    "label_init = torch.stack(label_embeds).to(device)\n",
    "print(f\"âœ“ Label embeddings: {label_init.shape}\")\n",
    "\n",
    "del bert\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:16.999131Z",
     "iopub.status.busy": "2025-12-19T13:24:16.998741Z",
     "iopub.status.idle": "2025-12-19T13:24:17.004489Z",
     "shell.execute_reply": "2025-12-19T13:24:17.003850Z",
     "shell.execute_reply.started": "2025-12-19T13:24:16.999116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Models defined\n"
     ]
    }
   ],
   "source": [
    "# GCN Model\n",
    "class LabelGCN(nn.Module):\n",
    "    def __init__(self, dim, layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.dropout = dropout\n",
    "        self.weights = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(dim, dim)) for _ in range(layers)\n",
    "        ])\n",
    "        for W in self.weights:\n",
    "            nn.init.xavier_uniform_(W)\n",
    "    \n",
    "    def forward(self, H, A):\n",
    "        for i, W in enumerate(self.weights):\n",
    "            H = torch.matmul(A, H)\n",
    "            H = torch.matmul(H, W)\n",
    "            if i < self.layers - 1:\n",
    "                H = F.relu(H)\n",
    "                H = F.dropout(H, p=self.dropout, training=self.training)\n",
    "        return H\n",
    "\n",
    "class LabelGCNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, label_emb, A, layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        emb_dim = label_emb.size(1)\n",
    "        \n",
    "        self.proj = nn.Linear(input_dim, emb_dim)\n",
    "        self.gcn = LabelGCN(emb_dim, layers, dropout)\n",
    "        self.label_emb = nn.Parameter(label_emb.clone())\n",
    "        self.register_buffer('A', A)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        refined = self.gcn(self.label_emb, self.A)\n",
    "        x_proj = self.proj(x)\n",
    "        x_proj = F.dropout(x_proj, p=self.dropout, training=self.training)\n",
    "        logits = torch.matmul(x_proj, refined.T)\n",
    "        return logits\n",
    "\n",
    "print(\"âœ“ Models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:18.263468Z",
     "iopub.status.busy": "2025-12-19T13:24:18.263240Z",
     "iopub.status.idle": "2025-12-19T13:24:18.277631Z",
     "shell.execute_reply": "2025-12-19T13:24:18.276997Z",
     "shell.execute_reply.started": "2025-12-19T13:24:18.263449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model: 2,178,048 params\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = LabelGCNClassifier(\n",
    "    input_dim=HIDDEN_DIM,\n",
    "    label_emb=label_init,\n",
    "    A=A_hat,\n",
    "    layers=GCN_LAYERS,\n",
    "    dropout=GCN_DROPOUT\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ“ Model: {sum(p.numel() for p in model.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:24:19.327454Z",
     "iopub.status.busy": "2025-12-19T13:24:19.327024Z",
     "iopub.status.idle": "2025-12-19T13:26:14.871399Z",
     "shell.execute_reply": "2025-12-19T13:26:14.870066Z",
     "shell.execute_reply.started": "2025-12-19T13:24:19.327415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test corpus...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19658/19658 [01:55<00:00, 170.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Encoded\n",
      "CPU times: user 1min 55s, sys: 524 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Encode test corpus\n",
    "print(\"Encoding test corpus...\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "bert = AutoModel.from_pretrained(BERT_MODEL).to(device)\n",
    "bert.eval()\n",
    "\n",
    "test_embs = {}\n",
    "pids = list(test_corpus.keys())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pid in tqdm(pids, desc=\"Encoding\"):\n",
    "        text = test_corpus[pid]\n",
    "        enc = tokenizer(text, return_tensors='pt', truncation=True,\n",
    "                       max_length=256, padding=True)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        \n",
    "        out = bert(**enc)\n",
    "        emb = out.last_hidden_state.mean(dim=1).squeeze()\n",
    "        test_embs[pid] = emb.cpu()\n",
    "\n",
    "print(\"âœ“ Encoded\")\n",
    "\n",
    "del bert\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:26:37.909429Z",
     "iopub.status.busy": "2025-12-19T13:26:37.909195Z",
     "iopub.status.idle": "2025-12-19T13:26:45.262121Z",
     "shell.execute_reply": "2025-12-19T13:26:45.261435Z",
     "shell.execute_reply.started": "2025-12-19T13:26:37.909411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19658/19658 [00:07<00:00, 2675.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Done\n",
      "CPU times: user 7.34 s, sys: 28.1 ms, total: 7.37 s\n",
      "Wall time: 7.35 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Predict\n",
    "print(\"Predicting...\\n\")\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pid in tqdm(pids, desc=\"Predict\"):\n",
    "        emb = test_embs[pid].unsqueeze(0).to(device)\n",
    "        logits = model(emb).squeeze()\n",
    "        scores = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        # Top K\n",
    "        top_k = np.argsort(scores)[::-1][:TOP_K]\n",
    "        \n",
    "        # Filter\n",
    "        cands = [(int(i), scores[i]) for i in top_k if scores[i] >= MIN_SCORE]\n",
    "        \n",
    "        if len(cands) < 2:\n",
    "            cands = [(int(i), scores[i]) for i in top_k[:3]]\n",
    "        \n",
    "        predictions[pid] = [c[0] for c in cands[:3]]\n",
    "\n",
    "print(\"âœ“ Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:27:13.505493Z",
     "iopub.status.busy": "2025-12-19T13:27:13.505256Z",
     "iopub.status.idle": "2025-12-19T13:27:13.515006Z",
     "shell.execute_reply": "2025-12-19T13:27:13.514329Z",
     "shell.execute_reply.started": "2025-12-19T13:27:13.505476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "Unique classes: 161/531\n",
      "Expected: 0.32-0.45\n",
      "\n",
      "Min: 1, Max: 15436, Mean: 366.3\n",
      "\n",
      "Top 10:\n",
      "  toys_games                              : 15436 (78.5%)\n",
      "  baby_products                           : 5828 (29.6%)\n",
      "  hair_perms_texturizers                  : 1667 (8.5%)\n",
      "  styling_tools                           : 1615 (8.2%)\n",
      "  styling_products                        : 1615 (8.2%)\n",
      "  hair_loss_products                      : 1591 (8.1%)\n",
      "  shampoos                                : 1574 (8.0%)\n",
      "  hair_color                              : 1530 (7.8%)\n",
      "  hair_relaxers                           : 1522 (7.7%)\n",
      "  hair_scalp_treatments                   : 1500 (7.6%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "all_classes = []\n",
    "for labels in predictions.values():\n",
    "    all_classes.extend(labels)\n",
    "counts = Counter(all_classes)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Unique classes: {len(counts)}/531\")\n",
    "\n",
    "if len(counts) >= 250:\n",
    "    print(f\"Expected: 0.40-0.55 âœ“\")\n",
    "elif len(counts) >= 150:\n",
    "    print(f\"Expected: 0.32-0.45\")\n",
    "else:\n",
    "    print(f\"Expected: 0.25-0.35\")\n",
    "\n",
    "vals = list(counts.values())\n",
    "print(f\"\\nMin: {min(vals)}, Max: {max(vals)}, Mean: {np.mean(vals):.1f}\")\n",
    "\n",
    "print(f\"\\nTop 10:\")\n",
    "for cid, cnt in counts.most_common(10):\n",
    "    print(f\"  {id2class[cid][:40]:40s}: {cnt:4d} ({cnt/len(predictions)*100:.1f}%)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T13:27:16.504700Z",
     "iopub.status.busy": "2025-12-19T13:27:16.504474Z",
     "iopub.status.idle": "2025-12-19T13:27:16.536469Z",
     "shell.execute_reply": "2025-12-19T13:27:16.535808Z",
     "shell.execute_reply.started": "2025-12-19T13:27:16.504684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Saved: outputs/label_gcn_predictions.csv\n",
      "\n",
      "Sample:\n",
      "   id      labels\n",
      "0   0    3,80,461\n",
      "1   1    3,90,473\n",
      "2   2     3,40,46\n",
      "3   3    40,3,205\n",
      "4   4     3,40,90\n",
      "5   5  40,169,442\n",
      "6   6    3,90,338\n",
      "7   7   3,237,143\n",
      "8   8   3,461,335\n",
      "9   9   3,116,143\n",
      "\n",
      "============================================================\n",
      "LABEL GCN COMPLETE\n",
      "============================================================\n",
      "Unique: 161/531\n",
      "\n",
      "ðŸ“¤ Submit to Kaggle\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "out = os.path.join(OUTPUT_DIR, 'label_gcn_predictions.csv')\n",
    "\n",
    "with open(out, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'labels'])\n",
    "    for pid in sorted(predictions.keys(), key=lambda x: int(x)):\n",
    "        writer.writerow([pid, ','.join(map(str, predictions[pid]))])\n",
    "\n",
    "print(f\"\\nâœ“ Saved: {out}\")\n",
    "\n",
    "df = pd.read_csv(out)\n",
    "print(\"\\nSample:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LABEL GCN COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Unique: {len(counts)}/531\")\n",
    "print(f\"\\nðŸ“¤ Submit to Kaggle\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
