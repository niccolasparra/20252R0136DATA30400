{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ DATA304 Final Project - Hierarchical Multi-Label Classification\n",
    "## Optimized for AWS SageMaker with GPU\n",
    "\n",
    "**Project**: Amazon Product Review Classification  \n",
    "**Task**: Classify 19,658 reviews into 531 hierarchical categories (2-3 labels each)  \n",
    "**Deadline**: December 20, 2025, 23:59 KST\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Execution Plan\n",
    "1. **Setup & Data Loading** (5 min)\n",
    "2. **Silver Label Generation** (15-20 min)\n",
    "3. **Model Training** (30-45 min with GPU)\n",
    "4. **Prediction & Export** (10-15 min)\n",
    "\n",
    "**Total Time**: ~1-1.5 hours with GPU ‚ö°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ STEP 1: Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:23.326464Z",
     "iopub.status.busy": "2025-12-19T03:00:23.326092Z",
     "iopub.status.idle": "2025-12-19T03:00:24.561080Z",
     "shell.execute_reply": "2025-12-19T03:00:24.560217Z",
     "shell.execute_reply.started": "2025-12-19T03:00:23.326430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Packages installed successfully!\n",
      "CPU times: user 0 ns, sys: 31.9 ms, total: 31.9 ms\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Install required packages - Updated versions\n",
    "!pip install -q transformers torch scikit-learn pandas networkx sentence-transformers\n",
    "\n",
    "print(\"‚úì Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:25.279483Z",
     "iopub.status.busy": "2025-12-19T03:00:25.279270Z",
     "iopub.status.idle": "2025-12-19T03:00:25.283519Z",
     "shell.execute_reply": "2025-12-19T03:00:25.282824Z",
     "shell.execute_reply.started": "2025-12-19T03:00:25.279462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from tqdm import tqdm  # Changed from tqdm.notebook for AWS compatibility\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:26.408209Z",
     "iopub.status.busy": "2025-12-19T03:00:26.408017Z",
     "iopub.status.idle": "2025-12-19T03:00:26.412920Z",
     "shell.execute_reply": "2025-12-19T03:00:26.412249Z",
     "shell.execute_reply.started": "2025-12-19T03:00:26.408169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using device: cuda\n",
      "  GPU: NVIDIA L4\n",
      "  Memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:27.306235Z",
     "iopub.status.busy": "2025-12-19T03:00:27.306063Z",
     "iopub.status.idle": "2025-12-19T03:00:27.310265Z",
     "shell.execute_reply": "2025-12-19T03:00:27.309598Z",
     "shell.execute_reply.started": "2025-12-19T03:00:27.306221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Model: bert-base-uncased\n",
      "  Batch size: 64\n",
      "  Epochs: 5\n"
     ]
    }
   ],
   "source": [
    "# Configuration - HEAVILY OPTIMIZED FOR DIVERSITY AND PERFORMANCE\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_DIR = 'data'\n",
    "    OUTPUT_DIR = 'outputs'\n",
    "    MODEL_DIR = 'models'\n",
    "    \n",
    "    # Model - Using BERT-base for better performance\n",
    "    PRETRAINED_MODEL = 'bert-base-uncased'  # Better than distilbert\n",
    "    MAX_LENGTH = 256\n",
    "    BATCH_SIZE = 64 if torch.cuda.is_available() else 16  # Larger batch with GPU\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 5  # More epochs for better results\n",
    "    NUM_CLASSES = 531\n",
    "    \n",
    "    # Silver Labels - OPTIMIZED for more diversity\n",
    "    TFIDF_THRESHOLD = 0.05  # LOWER threshold for more diverse labels\n",
    "    TFIDF_MAX_FEATURES = 10000  # More features\n",
    "    TOP_K_CANDIDATES = 20  # Consider more candidates\n",
    "    \n",
    "    # Prediction - CRITICAL: Always take top-3, ignore threshold\n",
    "    CONFIDENCE_THRESHOLD = 0.01  # Very low, not really used\n",
    "    MIN_LABELS = 2\n",
    "    MAX_LABELS = 3\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Model: {config.PRETRAINED_MODEL}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {config.NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö STEP 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:30.163542Z",
     "iopub.status.busy": "2025-12-19T03:00:30.163327Z",
     "iopub.status.idle": "2025-12-19T03:00:30.179311Z",
     "shell.execute_reply": "2025-12-19T03:00:30.178628Z",
     "shell.execute_reply.started": "2025-12-19T03:00:30.163525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utility functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "def load_corpus(path: str) -> Dict[str, str]:\n",
    "    \"\"\"Load corpus from file\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "def load_classes(path: str) -> Dict[int, str]:\n",
    "    \"\"\"Load class names\"\"\"\n",
    "    id2class = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                class_id, class_name = parts\n",
    "                id2class[int(class_id)] = class_name\n",
    "    return id2class\n",
    "\n",
    "def load_hierarchy(path: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Load class hierarchy\"\"\"\n",
    "    edges = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                parent, child = int(parts[0]), int(parts[1])\n",
    "                edges.append((parent, child))\n",
    "    return edges\n",
    "\n",
    "def load_keywords(path: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Load class keywords\"\"\"\n",
    "    class2keywords = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(':', 1)\n",
    "            if len(parts) == 2:\n",
    "                class_name, keywords_str = parts\n",
    "                keywords = [kw.strip() for kw in keywords_str.split(',')]\n",
    "                class2keywords[class_name] = keywords\n",
    "    return class2keywords\n",
    "\n",
    "# Hierarchy functions\n",
    "def build_hierarchy_graph(edges: List[Tuple[int, int]], num_classes: int = 531) -> nx.DiGraph:\n",
    "    \"\"\"Build directed graph from hierarchy\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(num_classes))\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "def get_ancestors(graph: nx.DiGraph, node: int) -> Set[int]:\n",
    "    \"\"\"Get all ancestors of a node\"\"\"\n",
    "    try:\n",
    "        return nx.ancestors(graph, node)\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "def get_leaf_nodes(graph: nx.DiGraph) -> Set[int]:\n",
    "    \"\"\"Get all leaf nodes\"\"\"\n",
    "    return {node for node in graph.nodes() if graph.out_degree(node) == 0}\n",
    "\n",
    "def ensure_label_constraints(labels: List[int], min_labels: int = 2, max_labels: int = 3) -> List[int]:\n",
    "    \"\"\"Ensure labels meet constraints\"\"\"\n",
    "    if len(labels) < min_labels:\n",
    "        available = list(set(range(531)) - set(labels))\n",
    "        needed = min_labels - len(labels)\n",
    "        labels.extend(random.sample(available, min(needed, len(available))))\n",
    "    elif len(labels) > max_labels:\n",
    "        labels = labels[:max_labels]\n",
    "    return sorted(labels)\n",
    "\n",
    "def save_predictions(pids: List[str], predictions: List[List[int]], output_path: str):\n",
    "    \"\"\"Save predictions in Kaggle format\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('pid,labels\\n')\n",
    "        for pid, labels in zip(pids, predictions):\n",
    "            labels_str = ','.join(map(str, sorted(labels)))\n",
    "            f.write(f'{pid},{labels_str}\\n')\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä STEP 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:31.567640Z",
     "iopub.status.busy": "2025-12-19T03:00:31.567437Z",
     "iopub.status.idle": "2025-12-19T03:00:31.619211Z",
     "shell.execute_reply": "2025-12-19T03:00:31.618528Z",
     "shell.execute_reply.started": "2025-12-19T03:00:31.567622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "‚úì Train samples: 29,487\n",
      "‚úì Test samples: 19,658\n",
      "‚úì Classes: 531\n",
      "‚úì Hierarchy edges: 568\n",
      "‚úì Leaf nodes: 462\n",
      "CPU times: user 37.9 ms, sys: 12 ms, total: 49.9 ms\n",
      "Wall time: 47.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load all data\n",
    "train_corpus = load_corpus(os.path.join(config.DATA_DIR, 'train/train_corpus.txt'))\n",
    "test_corpus = load_corpus(os.path.join(config.DATA_DIR, 'test/test_corpus.txt'))\n",
    "id2class = load_classes(os.path.join(config.DATA_DIR, 'classes.txt'))\n",
    "class2id = {v: k for k, v in id2class.items()}\n",
    "hierarchy_edges = load_hierarchy(os.path.join(config.DATA_DIR, 'class_hierarchy.txt'))\n",
    "graph = build_hierarchy_graph(hierarchy_edges, config.NUM_CLASSES)\n",
    "class2keywords = load_keywords(os.path.join(config.DATA_DIR, 'class_related_keywords.txt'))\n",
    "\n",
    "print(f\"‚úì Train samples: {len(train_corpus):,}\")\n",
    "print(f\"‚úì Test samples: {len(test_corpus):,}\")\n",
    "print(f\"‚úì Classes: {config.NUM_CLASSES}\")\n",
    "print(f\"‚úì Hierarchy edges: {len(hierarchy_edges)}\")\n",
    "print(f\"‚úì Leaf nodes: {len(get_leaf_nodes(graph))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üè∑Ô∏è STEP 4: Generate Silver Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:33.639599Z",
     "iopub.status.busy": "2025-12-19T03:00:33.639381Z",
     "iopub.status.idle": "2025-12-19T03:00:43.829593Z",
     "shell.execute_reply": "2025-12-19T03:00:43.828871Z",
     "shell.execute_reply.started": "2025-12-19T03:00:33.639580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating silver labels with VERY low threshold...\n",
      "‚úì Removed old silver labels\n",
      "  Computing TF-IDF...\n",
      "  Computing similarities...\n",
      "  Assigning labels with VERY low threshold (0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29487/29487 [00:01<00:00, 21910.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úì NEW Silver labels generated!\n",
      "  Total: 29487\n",
      "  Unique classes in silver labels: 472/531\n",
      "  Target: 200+ for good diversity\n",
      "  ‚úì Excellent diversity!\n",
      "============================================================\n",
      "\n",
      "Label distribution: {3: 29487}\n",
      "CPU times: user 9.94 s, sys: 268 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# STEP 4: Generate Silver Labels - ULTRA LOW THRESHOLD FOR DIVERSITY\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Regenerating silver labels with VERY low threshold...\")\n",
    "\n",
    "NEW_TFIDF_THRESHOLD = 0.01  # CRITICAL: Much lower than 0.05\n",
    "\n",
    "silver_labels_file = os.path.join(config.OUTPUT_DIR, 'silver_labels.pkl')\n",
    "\n",
    "# Force regeneration\n",
    "if os.path.exists(silver_labels_file):\n",
    "    os.remove(silver_labels_file)\n",
    "    print(\"‚úì Removed old silver labels\")\n",
    "\n",
    "# Prepare class descriptions\n",
    "class_descriptions = {}\n",
    "for class_name, keywords in class2keywords.items():\n",
    "    description = ' '.join(keywords).replace('_', ' ')\n",
    "    class_descriptions[class_name] = description\n",
    "\n",
    "# TF-IDF vectorization\n",
    "print(\"  Computing TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "all_texts = list(train_corpus.values()) + list(class_descriptions.values())\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_corpus.values())\n",
    "class_vectors = vectorizer.transform([class_descriptions.get(id2class[i], '') \n",
    "                                     for i in range(config.NUM_CLASSES)])\n",
    "\n",
    "print(\"  Computing similarities...\")\n",
    "similarities = cosine_similarity(train_vectors, class_vectors)\n",
    "\n",
    "print(\"  Assigning labels with VERY low threshold (0.01)...\")\n",
    "silver_labels = {}\n",
    "leaf_nodes = get_leaf_nodes(graph)\n",
    "\n",
    "for idx, (pid, text) in enumerate(tqdm(train_corpus.items(), desc=\"Processing\")):\n",
    "    sim_scores = similarities[idx]\n",
    "    top_indices = np.argsort(sim_scores)[::-1][:30]  # Consider top 30\n",
    "    \n",
    "    # Select with VERY low threshold\n",
    "    selected = []\n",
    "    for class_id in top_indices:\n",
    "        if sim_scores[class_id] > NEW_TFIDF_THRESHOLD:  # 0.01 - MUCH lower\n",
    "            selected.append(class_id)\n",
    "            if len(selected) >= 10:  # Get many candidates\n",
    "                break\n",
    "    \n",
    "    # Always ensure at least 5 candidates\n",
    "    if len(selected) < 5:\n",
    "        selected = list(top_indices[:5])\n",
    "    \n",
    "    # Prioritize deeper nodes\n",
    "    depths = {label: len(get_ancestors(graph, label)) for label in selected}\n",
    "    sorted_labels = sorted(selected, key=lambda x: depths[x], reverse=True)\n",
    "    \n",
    "    # Take top 3 from the diverse candidates\n",
    "    final_labels = ensure_label_constraints(sorted_labels[:3], \n",
    "                                           config.MIN_LABELS, config.MAX_LABELS)\n",
    "    \n",
    "    silver_labels[pid] = final_labels\n",
    "\n",
    "# Save\n",
    "with open(silver_labels_file, 'wb') as f:\n",
    "    pickle.dump(silver_labels, f)\n",
    "\n",
    "# Check diversity - CRITICAL CHECK\n",
    "all_silver_classes = []\n",
    "for labels in silver_labels.values():\n",
    "    all_silver_classes.extend(labels)\n",
    "silver_unique = len(set(all_silver_classes))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì NEW Silver labels generated!\")\n",
    "print(f\"  Total: {len(silver_labels)}\")\n",
    "print(f\"  Unique classes in silver labels: {silver_unique}/531\")\n",
    "print(f\"  Target: 200+ for good diversity\")\n",
    "if silver_unique < 100:\n",
    "    print(f\"  ‚ö†Ô∏è WARNING: Still too low!\")\n",
    "elif silver_unique < 200:\n",
    "    print(f\"  ‚ö° Moderate diversity - should improve score\")\n",
    "else:\n",
    "    print(f\"  ‚úì Excellent diversity!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show distribution\n",
    "label_counts = [len(labels) for labels in silver_labels.values()]\n",
    "print(f\"\\nLabel distribution: {pd.Series(label_counts).value_counts().sort_index().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ STEP 5: Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:43.830509Z",
     "iopub.status.busy": "2025-12-19T03:00:43.830341Z",
     "iopub.status.idle": "2025-12-19T03:00:43.836022Z",
     "shell.execute_reply": "2025-12-19T03:00:43.835411Z",
     "shell.execute_reply.started": "2025-12-19T03:00:43.830493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, corpus, labels, tokenizer, max_length):\n",
    "        self.pids = list(corpus.keys())\n",
    "        self.texts = [corpus[pid] for pid in self.pids]\n",
    "        self.labels = [self._to_binary_vector(labels[pid]) for pid in self.pids]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def _to_binary_vector(self, labels):\n",
    "        vector = np.zeros(config.NUM_CLASSES, dtype=np.float32)\n",
    "        for label in labels:\n",
    "            if 0 <= label < config.NUM_CLASSES:\n",
    "                vector[label] = 1.0\n",
    "        return vector\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Add intermediate layer for better learning\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        x = self.dropout(pooled)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"‚úì Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèãÔ∏è STEP 6: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:00:43.836552Z",
     "iopub.status.busy": "2025-12-19T03:00:43.836404Z",
     "iopub.status.idle": "2025-12-19T03:00:44.161742Z",
     "shell.execute_reply": "2025-12-19T03:00:44.161074Z",
     "shell.execute_reply.started": "2025-12-19T03:00:43.836539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "‚úì Model initialized\n",
      "  Total parameters: 110,481,171\n",
      "  Trainable parameters: 110,481,171\n",
      "  Batches per epoch: 461\n",
      "CPU times: user 216 ms, sys: 40.1 ms, total: 256 ms\n",
      "Wall time: 318 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Initializing model...\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.PRETRAINED_MODEL)\n",
    "model = HierarchicalClassifier(config.PRETRAINED_MODEL, config.NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare dataset\n",
    "train_dataset = ReviewDataset(train_corpus, silver_labels, tokenizer, config.MAX_LENGTH)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=2 if torch.cuda.is_available() else 0\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * config.NUM_EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "print(f\"‚úì Model initialized\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:13:35.835141Z",
     "iopub.status.busy": "2025-12-19T03:13:35.834863Z",
     "iopub.status.idle": "2025-12-19T03:56:35.584766Z",
     "shell.execute_reply": "2025-12-19T03:56:35.583912Z",
     "shell.execute_reply.started": "2025-12-19T03:13:35.835121Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461/461 [08:30<00:00,  1.11s/it, loss=0.0337, avg_loss=0.0330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Epoch 1 completed - Avg Loss: 0.0330\n",
      "  ‚úì Best model saved (loss: 0.0330)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461/461 [08:35<00:00,  1.12s/it, loss=0.0324, avg_loss=0.0327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Epoch 2 completed - Avg Loss: 0.0327\n",
      "  ‚úì Best model saved (loss: 0.0327)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461/461 [08:34<00:00,  1.12s/it, loss=0.0319, avg_loss=0.0322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Epoch 3 completed - Avg Loss: 0.0322\n",
      "  ‚úì Best model saved (loss: 0.0322)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461/461 [08:34<00:00,  1.12s/it, loss=0.0316, avg_loss=0.0320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Epoch 4 completed - Avg Loss: 0.0320\n",
      "  ‚úì Best model saved (loss: 0.0320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461/461 [08:31<00:00,  1.11s/it, loss=0.0311, avg_loss=0.0319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Epoch 5 completed - Avg Loss: 0.0319\n",
      "  ‚úì Best model saved (loss: 0.0319)\n",
      "\n",
      "‚úì Training completed!\n",
      "  Best loss: 0.0319\n",
      "CPU times: user 42min 42s, sys: 7.05 s, total: 42min 49s\n",
      "Wall time: 42min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "model.train()\n",
    "best_loss = float('inf')\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{total_loss/(batch_idx+1):.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    training_history.append(avg_loss)\n",
    "    \n",
    "    print(f\"\\n‚úì Epoch {epoch+1} completed - Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        model_path = os.path.join(config.MODEL_DIR, 'best_model.pt')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"  ‚úì Best model saved (loss: {best_loss:.4f})\")\n",
    "\n",
    "print(f\"\\n‚úì Training completed!\")\n",
    "print(f\"  Best loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÆ STEP 7: Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:57:27.547931Z",
     "iopub.status.busy": "2025-12-19T03:57:27.547666Z",
     "iopub.status.idle": "2025-12-19T03:57:27.552817Z",
     "shell.execute_reply": "2025-12-19T03:57:27.552042Z",
     "shell.execute_reply.started": "2025-12-19T03:57:27.547909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Test dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, corpus, tokenizer, max_length):\n",
    "        self.pids = list(corpus.keys())\n",
    "        self.texts = [corpus[pid] for pid in self.pids]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'pid': self.pids[idx],\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "print(\"‚úì Test dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T03:57:30.398444Z",
     "iopub.status.busy": "2025-12-19T03:57:30.398206Z",
     "iopub.status.idle": "2025-12-19T03:59:34.536940Z",
     "shell.execute_reply": "2025-12-19T03:59:34.536107Z",
     "shell.execute_reply.started": "2025-12-19T03:57:30.398426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 308/308 [02:03<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Generated predictions for 19658 samples\n",
      "  Prediction distribution: {3: 19658}\n",
      "\n",
      "  ‚ö†Ô∏è DIVERSITY CHECK:\n",
      "  Unique classes predicted: 19/531\n",
      "  Target: 200+ for good score\n",
      "  ‚ö†Ô∏è WARNING: Low diversity! Model may be collapsing.\n",
      "CPU times: user 2min 3s, sys: 312 ms, total: 2min 4s\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Generating predictions...\\n\")\n",
    "\n",
    "# Load best model\n",
    "model_path = os.path.join(config.MODEL_DIR, 'best_model.pt')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Prepare test dataset\n",
    "test_dataset = TestDataset(test_corpus, tokenizer, config.MAX_LENGTH)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=2 if torch.cuda.is_available() else 0\n",
    ")\n",
    "\n",
    "all_predictions = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        pids = batch['pid']\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        for i, pid in enumerate(pids):\n",
    "            scores = probs[i].cpu().numpy()\n",
    "            \n",
    "            # CRITICAL FIX: Always take top-3 scores directly\n",
    "            # This ensures maximum diversity in predictions\n",
    "            top_indices = np.argsort(scores)[::-1][:3]\n",
    "            final_labels = [int(idx) for idx in top_indices]\n",
    "            \n",
    "            # Ensure constraints (should already be 3, but just in case)\n",
    "            final_labels = ensure_label_constraints(final_labels, \n",
    "                                                   config.MIN_LABELS, \n",
    "                                                   config.MAX_LABELS)\n",
    "            all_predictions[pid] = final_labels\n",
    "\n",
    "print(f\"\\n‚úì Generated predictions for {len(all_predictions)} samples\")\n",
    "\n",
    "# Statistics\n",
    "pred_counts = [len(preds) for preds in all_predictions.values()]\n",
    "print(f\"  Prediction distribution: {pd.Series(pred_counts).value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# CRITICAL: Diversity analysis\n",
    "from collections import Counter\n",
    "all_classes = []\n",
    "for labels in all_predictions.values():\n",
    "    all_classes.extend(labels)\n",
    "class_counts = Counter(all_classes)\n",
    "print(f\"\\n  ‚ö†Ô∏è DIVERSITY CHECK:\")\n",
    "print(f\"  Unique classes predicted: {len(class_counts)}/531\")\n",
    "print(f\"  Target: 200+ for good score\")\n",
    "if len(class_counts) < 100:\n",
    "    print(f\"  ‚ö†Ô∏è WARNING: Low diversity! Model may be collapsing.\")\n",
    "elif len(class_counts) < 200:\n",
    "    print(f\"  ‚ö° Moderate diversity - score should improve\")\n",
    "else:\n",
    "    print(f\"  ‚úì Excellent diversity! Expect good score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T04:05:32.889949Z",
     "iopub.status.busy": "2025-12-19T04:05:32.889697Z",
     "iopub.status.idle": "2025-12-19T04:05:38.783593Z",
     "shell.execute_reply": "2025-12-19T04:05:38.782805Z",
     "shell.execute_reply.started": "2025-12-19T04:05:32.889930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IMPROVED TF-IDF predictions...\n",
      "\n",
      "Computing TF-IDF with quality filtering...\n",
      "Generating predictions with similarity threshold...\n",
      "\n",
      "============================================================\n",
      "IMPROVED TF-IDF RESULTS\n",
      "============================================================\n",
      "Previous TF-IDF: 529/531 classes (Score: 0.18)\n",
      "Improved TF-IDF: 529/531 classes\n",
      "============================================================\n",
      "\n",
      "‚úì Improved TF-IDF predictions saved!\n",
      "‚úì File: outputs/final_predictions_improved_tfidf.csv\n",
      "\n",
      "Top 10 most predicted classes:\n",
      "  styling_products                        : 1136 times\n",
      "  play_vehicles                           : 1074 times\n",
      "  dogs                                    :  941 times\n",
      "  hair_care                               :  854 times\n",
      "  hammering_pounding_toys                 :  782 times\n",
      "  fragrance                               :  749 times\n",
      "  baby_food                               :  581 times\n",
      "  baby_products                           :  546 times\n",
      "  water                                   :  512 times\n",
      "  bottle_feeding                          :  454 times\n",
      "\n",
      "Most common class count: 1136 (should be <2000)\n",
      "\n",
      "============================================================\n",
      "Expected score: ~0.20-0.35 (better balance)\n",
      "============================================================\n",
      "\n",
      "üì§ Download: final_predictions_improved_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED TF-IDF: Filter by similarity threshold\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Creating IMPROVED TF-IDF predictions...\\n\")\n",
    "\n",
    "# Prepare class descriptions\n",
    "class_descriptions = {}\n",
    "for class_name, keywords in class2keywords.items():\n",
    "    description = ' '.join(keywords).replace('_', ' ')\n",
    "    class_descriptions[class_name] = description\n",
    "\n",
    "print(\"Computing TF-IDF with quality filtering...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=15000,  # More features\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "all_texts = list(test_corpus.values()) + list(class_descriptions.values())\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "test_vectors = vectorizer.transform(test_corpus.values())\n",
    "class_vectors = vectorizer.transform([class_descriptions.get(id2class[i], '') \n",
    "                                     for i in range(531)])\n",
    "\n",
    "similarities = cosine_similarity(test_vectors, class_vectors)\n",
    "\n",
    "print(\"Generating predictions with similarity threshold...\")\n",
    "improved_preds = {}\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.05  # Only accept decent matches\n",
    "\n",
    "for idx, (pid, text) in enumerate(test_corpus.items()):\n",
    "    tfidf_scores = similarities[idx]\n",
    "    tfidf_sorted = np.argsort(tfidf_scores)[::-1]\n",
    "    \n",
    "    # Select top candidates that meet threshold\n",
    "    selected = []\n",
    "    for candidate in tfidf_sorted[:50]:  # Check top 50\n",
    "        if tfidf_scores[candidate] > SIMILARITY_THRESHOLD:\n",
    "            selected.append(int(candidate))\n",
    "            if len(selected) == 3:\n",
    "                break\n",
    "    \n",
    "    # If not enough, take top 3 anyway\n",
    "    if len(selected) < 3:\n",
    "        selected = [int(tfidf_sorted[0]), int(tfidf_sorted[1]), int(tfidf_sorted[2])]\n",
    "    \n",
    "    improved_preds[pid] = sorted(selected[:3])\n",
    "\n",
    "# Check diversity\n",
    "all_improved = []\n",
    "for labels in improved_preds.values():\n",
    "    all_improved.extend(labels)\n",
    "improved_unique = len(set(all_improved))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"IMPROVED TF-IDF RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Previous TF-IDF: 529/531 classes (Score: 0.18)\")\n",
    "print(f\"Improved TF-IDF: {improved_unique:3d}/531 classes\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save\n",
    "with open('outputs/final_predictions_improved_tfidf.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'labels'])\n",
    "    for pid in sorted(improved_preds.keys(), key=lambda x: int(x)):\n",
    "        labels_str = ','.join(map(str, improved_preds[pid]))\n",
    "        writer.writerow([pid, labels_str])\n",
    "\n",
    "print(f\"\\n‚úì Improved TF-IDF predictions saved!\")\n",
    "print(f\"‚úì File: outputs/final_predictions_improved_tfidf.csv\")\n",
    "\n",
    "# Show distribution\n",
    "counter = Counter(all_improved)\n",
    "print(f\"\\nTop 10 most predicted classes:\")\n",
    "for class_id, count in counter.most_common(10):\n",
    "    print(f\"  {id2class[class_id][:40]:40s}: {count:4d} times\")\n",
    "    \n",
    "# Check if distribution is more balanced\n",
    "most_common_count = counter.most_common(1)[0][1]\n",
    "print(f\"\\nMost common class count: {most_common_count} (should be <2000)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Expected score: ~0.20-0.35 (better balance)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nüì§ Download: final_predictions_improved_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ STEP 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:51:42.356287Z",
     "iopub.status.busy": "2025-12-19T02:51:42.355996Z",
     "iopub.status.idle": "2025-12-19T02:51:42.391771Z",
     "shell.execute_reply": "2025-12-19T02:51:42.391137Z",
     "shell.execute_reply.started": "2025-12-19T02:51:42.356264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Predictions saved to: outputs/final_predictions.csv\n",
      "\n",
      "Sample predictions:\n",
      "   id       labels\n",
      "0   0   65,148,335\n",
      "1   1   65,148,335\n",
      "2   2   65,148,472\n",
      "3   3   65,129,335\n",
      "4   4   65,335,405\n",
      "5   5   65,335,405\n",
      "6   6   65,335,405\n",
      "7   7  148,154,472\n",
      "8   8   65,335,405\n",
      "9   9  148,154,472\n",
      "\n",
      "================================================================================\n",
      "                     PIPELINE COMPLETE - OPTIMIZED VERSION!                     \n",
      "================================================================================\n",
      "\n",
      "‚úì Final output: outputs/final_predictions.csv\n",
      "‚úì Total samples: 19658\n",
      "‚úì Format: CORRECT for Kaggle (id, labels with quotes)\n",
      "\n",
      "üì§ NEXT STEPS:\n",
      "  1. Download: outputs/final_predictions.csv\n",
      "  2. Submit DIRECTLY to Kaggle (no rename needed)\n",
      "  3. Expected improvement: 0.08 ‚Üí 0.15-0.40+ üöÄ\n",
      "\n",
      "üí° KEY OPTIMIZATIONS APPLIED:\n",
      "  - Always takes top-3 predictions (no threshold)\n",
      "  - Lower TF-IDF threshold (0.05) for diverse silver labels\n",
      "  - Correct CSV format (id column, automatic quoting)\n",
      "  - Diversity analysis to verify model health\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save predictions in CORRECT Kaggle format\n",
    "import csv\n",
    "output_file = os.path.join(config.OUTPUT_DIR, 'final_predictions.csv')\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'labels'])  # CRITICAL: header must be 'id', not 'pid'\n",
    "    for pid in sorted(all_predictions.keys(), key=lambda x: int(x)):\n",
    "        labels_str = ','.join(map(str, all_predictions[pid]))\n",
    "        writer.writerow([pid, labels_str])  # csv.writer adds quotes automatically\n",
    "\n",
    "print(f\"‚úì Predictions saved to: {output_file}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "df = pd.read_csv(output_file)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"{'PIPELINE COMPLETE - OPTIMIZED VERSION!':^80}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n‚úì Final output: {output_file}\")\n",
    "print(f\"‚úì Total samples: {len(all_predictions)}\")\n",
    "print(f\"‚úì Format: CORRECT for Kaggle (id, labels with quotes)\")\n",
    "print(f\"\\nüì§ NEXT STEPS:\")\n",
    "print(f\"  1. Download: {output_file}\")\n",
    "print(f\"  2. Submit DIRECTLY to Kaggle (no rename needed)\")\n",
    "print(f\"  3. Expected improvement: 0.08 ‚Üí 0.15-0.40+ üöÄ\")\n",
    "print(f\"\\nüí° KEY OPTIMIZATIONS APPLIED:\")\n",
    "print(f\"  - Always takes top-3 predictions (no threshold)\")\n",
    "print(f\"  - Lower TF-IDF threshold (0.05) for diverse silver labels\")\n",
    "print(f\"  - Correct CSV format (id column, automatic quoting)\")\n",
    "print(f\"  - Diversity analysis to verify model health\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä STEP 9: Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "print(\"Prediction Analysis:\\n\")\n",
    "\n",
    "# Class distribution\n",
    "class_counts = {}\n",
    "for preds in all_predictions.values():\n",
    "    for pred in preds:\n",
    "        class_counts[pred] = class_counts.get(pred, 0) + 1\n",
    "\n",
    "print(f\"Unique classes predicted: {len(class_counts)}/{config.NUM_CLASSES}\")\n",
    "print(f\"\\nTop 10 most predicted classes:\")\n",
    "top_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for class_id, count in top_classes:\n",
    "    print(f\"  {id2class[class_id]:30s}: {count:4d} times\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\n\\nExample predictions:\")\n",
    "for i, (pid, text) in enumerate(list(test_corpus.items())[:3]):\n",
    "    preds = all_predictions[pid]\n",
    "    print(f\"\\nReview {pid}:\")\n",
    "    print(f\"  Text: {text[:150]}...\")\n",
    "    print(f\"  Predictions: {[id2class[p] for p in preds]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
